{
  "hash": "9b5406755e63a19a71549877d315c770",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text Analysis - Use N-Gram Graph\"\ncategories: [Text-Analysis, Statistic, Experiment]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(igraph)\nlibrary(tidytext)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"set up and load data\"}\ngtr_desc = read_parquet(\"data/gtr.parquet\") |> \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |> \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |> \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |>\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |> \n      str_extract(\"\\\\d+\") |> \n      as.numeric() |> \n      coalesce(0)\n  ) |> \n  # filter(is_partial) |> \n  group_by(project_ref) |>\n  mutate(occurance = n()) |> \n  ungroup() |> \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n# gtr_pj |>\n#   group_by(occurance) |> \n#   summarise(n_projects=n())\n\n## find out about \nunique_prj = gtr_pj |>\n  relocate(ProjectReference, project_ref, id) |> \n  group_by(project_ref) |> \n  mutate(rn=row_number()) |> \n  filter(rn==1) |> \n  select(-rn) |> \n  ungroup()\n\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |> \n  group_by(abstractText) |> \n  mutate(n=n()) |> \n  filter(n!=1) |>\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |> \n  anti_join(repeated_text, by=\"id\") |> \n  mutate(year = lubridate::year(StartDate)) |>\n  inner_join(gtr_desc, by=\"id\")\n```\n:::\n\n\n\n## explore n-gram\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## break into bi-grame\nabstract_words = analysis_prj |>\n  unnest_tokens(word, abstractText, \"ngrams\",n=2, drop=T) |> \n  count(word,id, sort=T)\n\n## try this `bind_tf_idf` function\nword_distinct = abstract_words |> \n  bind_tf_idf(word,id, n)\n\n## convert bi-graph into network graph\npharases = word_distinct |> \n  arrange(desc(tf_idf)) |> \n  tidyr::separate(word, into=c(\"word1\",\"word2\"), sep=\" \") |> \n  anti_join(stop_words,c(\"word1\"=\"word\")) |> \n  anti_join(stop_words, c(\"word2\"=\"word\")) |> \n  filter(if_all(c(word1,word2), ~!stringr::str_detect(.x, \"\\\\d+\"))) |> \n  mutate(pharase = paste(word1, word2)) |> \n  filter(!word1 |> str_detect(\"^_\"))\n\nrequire(tidygraph)\n\n## try find graphical centroid of biggest graph\ncatch_pharase = pharases |> \n  group_by(word1, word2) |> \n  summarise(\n    occurance = n()\n  ) |> arrange(-occurance)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'word1'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\ncatch_pharase\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 215,740 × 3\n# Groups:   word1 [24,727]\n   word1        word2        occurance\n   <chr>        <chr>            <int>\n 1 artificial   intelligence      1813\n 2 machine      learning           809\n 3 intelligence ai                 400\n 4 real         time               392\n 5 project      aims               322\n 6 deep         learning           249\n 7 real         world              246\n 8 neural       networks           225\n 9 cutting      edge               220\n10 wide         range              217\n# ℹ 215,730 more rows\n```\n\n\n:::\n:::\n\nbi-graph truns out to be very useful.... you gets to understand \n\n\n\n**Eigen Centrality** \nRun every node through to\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_graph = catch_pharase |> \n  as_tbl_graph() |> \n  morph(to_components) |> \n  as_tibble() |> \n  mutate(dim = map_int(graph, ~length(.x))) |> \n  arrange(desc(dim))\n\nbiggest_g=word_graph |> purrr::pluck(\"graph\",1)\n\n## eigen centrality\n\ncentroid_score = biggest_g |> eigen_centrality() |> pluck(\"vector\")\ncentroid = which(centroid_score==max(centroid_score))\n\nbiggest_g |> \n  convert(to_local_neighborhood, centroid, 1) |>  \n  mutate(score = centroid_score[name]) |>\n  convert(to_subgraph, score > quantile(score, 0.99)) |> \n  activate(edges) |> \n  arrange(occurance) |> \n  filter(occurance > 10) |> \n  ggraph(layout=\"gem\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\",trans=\"log\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  ggtitle(\"By Eigenvector Centrality\") +\n  theme_void()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSubsetting by nodes\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), color =\n\"cyan4\", : Ignoring unknown parameters: `trans`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nAnygraphical based algorithmn is interesting here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## harmonic_centrality\ncache_file = \"cache/04-betweeness_score.RDS\"\nif(interactive()) {\n  betweeness_score = biggest_g |> \n  activate(edges) |>\n  betweenness(weights=E(biggest_g)$\"occurance\")\n  saveRDS(centroid_score, cache_file)\n} else {\n  betweeness_score=readRDS(cache_file)\n}\nbtw_centre = which(centroid_score==max(centroid_score))\n\nbiggest_g |> \n  # convert(to_local_neighborhood, btw_centre) |> \n  mutate(score = betweeness_score[name]) |> \n  convert(to_subgraph, score > quantile(score, 0.95)) |> \n  arrange(desc(score)) |> \n  filter(row_number() < 50) |> \n  activate(edges) |> \n  filter(occurance > 100) |> \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  theme_void() +\n  ggtitle(\"By Edge Betweeness, visualise top 0.01 % \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSubsetting by nodes\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\nThis results actually makes sense if you are looking at `quot` is a wild card\nthat can get about a lot of things. \n\nUp to interpretation.\n\nOkay other then key terms. Seems like the edge betweeness is good for point out \nadjusant words rather than identify patterns.\n\n\n\n## Suggestions\n\nIt maybe usefull to use n-gram to extract combined terms from `title` and bind this \nback into abstract `n-graph`. \nThen when we look at graph centrality again we remove any waild card.\n\nPharases may high high frequency of occurance but this does not means that they \nhave a higher *\"graphic degree\"*.\n\n## Follow up\n\nThe better way is actually filter a few top occuring terms. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncatch_pharase |> \n  filter(occurance > 100) |> \n  as_tbl_graph() |> \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),trans=\"log\",edge_colour =\"cyan4\") +\n  geom_node_text(aes(label=name),repel = T) + \n  geom_node_point(size=5) +\n  theme_void()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), trans =\n\"log\", : Ignoring unknown parameters: `trans`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}