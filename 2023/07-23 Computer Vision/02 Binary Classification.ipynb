{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little revision on `cross-entrophy`. Imformation entrophy is also modeled as 1/log(probability). They reason they do this is so that lower probability have higher imformation value.\n",
    "\n",
    "The function for binary-cross-entropy is given in keras. \n",
    "```py\n",
    "tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0.0,\n",
    "    axis=-1,\n",
    "    reduction=losses_utils.ReductionV2.AUTO,\n",
    "    name='binary_crossentropy' # here you can call this in compile?\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow think entrophy is 0.87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# you give these values\n",
    "# the value in y_pred is called logit. The log of the 'odd'. This \n",
    "# statistical treatment \"normalize\" percentage (but host 0.5 at the center)\n",
    "y_true = [0, 1, 0, 0]\n",
    "y_pred = [-18.6, 0.51, 2.94, -12.8] \n",
    "\n",
    "y_true=np.array(y_true)\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "import tensorflow as tf\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bce_p = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "entropy_base = bce(y_true, y_pred).numpy()\n",
    "print(f'Tensorflow think entrophy is {entropy_base:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy on p is about 0.87\n",
      "Entropy manually calculated is 0.87\n",
      "You've got the right number!\n"
     ]
    }
   ],
   "source": [
    "# Convert logit back to probability\n",
    "p = np.e**y_pred/(1 + np.e**y_pred)\n",
    "#print(np.round(p, 3))\n",
    "\n",
    "# comfirm logit is calculated correctly\n",
    "entropy0 = bce_p(y_true, p).numpy() # you should get about the same number as 0.8\n",
    "print(f'Entropy on p is about {entropy0:.2f}')\n",
    "\n",
    "# calculate cross entrophy from p\n",
    "entropy0 = -1/4 * np.sum(\n",
    "    (1 - y_true) * np.log(1 - p) +  y_true * np.log(p)\n",
    ")\n",
    "print(f'Entropy manually calculated is {entropy0:.2f}')\n",
    "if np.round(entropy0,2) == np.round(entropy_base, 2):\n",
    "    print(\"You've got the right number!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
