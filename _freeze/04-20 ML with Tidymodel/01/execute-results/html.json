{
  "hash": "95e8c11dac45d893851583334fe32912",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ML with Tidymodel\"\ncategories: [Machine-Learning, R, Basics]\nauthor: F.L\ndate: 2024-04-21\n---\n\n\n# Introduction\n\nThis notebook summaries key point from *Hadley Wickham's Tidy Model with R*. The book only covers basic usage of tidy-model and some other dimension reduction techniques.\n\nLink to the Book: [https://www.tmwr.org/](https://www.tmwr.org/workflows)\n\n## Introduction of Data\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(tidymodels)\ndata(\"ames\")\n\n## refer to tmap: https://r-tmap.github.io/tmap-book/visual-variables.html\n## for osm data: https://cran.r-project.org/web/packages/osmdata/vignettes/osmdata.html\n## for query streets: https://wiki.openstreetmap.org/wiki/Key%3ahighway\n\names_sf = sf::st_as_sf(ames,coords = c(\"Longitude\",\"Latitude\"), crs=4326)\names_bbox = sf::st_bbox(ames_sf)\nosm_streets = opq(bbox = ames_bbox) |> \n  add_osm_feature(key=\"highway\",value = c(\n                                          'secondary'\n                                          ,'primary'\n                                          ,'tertiary'\n                                          ,'unclassified'\n                                          ,'residential')) |> \n  # add_osm_feature(key=\"highway\",value = 'motorway') |> \n  osmdata_sf()\n\n## view a intersection\nstreets_sf = sf::st_intersection(sf::st_as_sfc(ames_bbox), osm_streets$osm_lines)\n\ntm_shape(streets_sf) + \n  tm_lines(col='grey') +\ntm_shape(ames_sf) + \n  tm_dots( shape = \"Lot_Shape\"\n          ,col=\"Neighborhood\"\n          ,style = \"cont\"\n          ,size=0.05\n          ,border.col=NA\n          ,border.lwd=0.01) + \n  tm_layout(legend.show=FALSE)\n```\n\n::: {.cell-output-display}\n![](01_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n`ames` familar with this data may come handy compare different model output later.\n\n::: columns\n::: {.column width=0.5}\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidymodels)\n\nggplot2::theme_set(theme_minimal())\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n```\n\n::: {.cell-output-display}\n![](01_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nFirst thing they want to tell you is the data is not normal so require you to normalise somehow.\n:::\n\n::: {.column width=0.5}\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](01_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\names <- ames |> mutate(Sale_Price = log10(Sale_Price))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ames |> \n#   head(1) |> \n#   glimpse()\n```\n:::\n\n\n## Spoil Alert\n\nFollowing code create a linear model.\nPrediction uses these variables: \n\n\n::: {.cell}\n\n```{.r .cell-code}\names |> \n  select(Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type, Latitude, Longitude) |> \n  slice_sample(n=1) |> \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1\nColumns: 6\n$ Neighborhood <fct> Sawyer\n$ Gr_Liv_Area  <int> 1034\n$ Year_Built   <int> 1978\n$ Bldg_Type    <fct> OneFam\n$ Latitude     <dbl> 42.03135\n$ Longitude    <dbl> -93.67272\n```\n\n\n:::\n:::\n\nTher are their transformation:\n\n- Neighborhood: convert low frequency one to \"other\", then make dummy varible\n- Gr_Liv_Area: log10 treatment\n- Year_Built: year\n- Bldg_Type: convert building type into dummy varible\n- Latitude: spine function treatment\n- Longitude: spine function treatment\n\n\n```{.r}\nlibrary(tidymodels)\ndata(ames)\n\n## Normalise Prediction\names <- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## Split Data Sets\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\n## Recipy for Preprocessing Data, Build receipy object\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n  \n## Linear Model\nlm_model <- linear_reg() %>% set_engine(\"lm\")\n\n## Finaly Evaluate Lazy Object\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model) %>% \n  add_recipe(ames_rec)\n\n## Fit a Model\nlm_fit <- fit(lm_wflow, ames_train)\n```\n\n\n# The basics\n\n## Splitting/Feature Selection/Create a \"Data Budget\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n:::\n\n\n### Simple 80-20 split\n\nThe basics is the same, split train test. For this purpose you are splitting the data by 80-20.\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_split <- rsample::initial_split(ames, prop = 0.80)\names_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<2344/586/2930>\n```\n\n\n:::\n:::\n\n\nRegards to spliting portion here is the advice from the Book:\n\n> A test set should be avoided only when the data are pathologically small.\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\ndim(ames_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2344   74\n```\n\n\n:::\n:::\n\n\n### Validation Split 60-20-20\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split <- rsample::initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Validation/Testing/Total>\n<1758/586/586/2930>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\names_train <- training(ames_val_split)\names_test <- testing(ames_val_split)\names_val <- validation(ames_val_split)\n```\n:::\n\n\n### Concepts\n\n-   *independent experimental unit*: (knowing database basic this is just matter of object uid versus alternate uid) for example, measuring one patient\n-   *multi-level-data/multiple rows per experimental unit*:\n\n> Data splitting should occur at the independent experimental unit level of the data!!!\n\n> Simple resampling across rows would lead to some data within an experimental unit being in the training set and others in the test set.\n\n### Pracrtical Implication\n\n-   the book admit the practice of train and split at first for a validation of the model but follow up using all the data point possible for a better estimation of data.\n\n## Fitting Model with Parsnip\n\n-   `linear_reg`\n-   `rand_forest`\n\n### Linear Regression Family\n\n-   `lm`\n-   `glmnet`: fits generalised linear and model via penalized maximum likelihood.\n-   `stan`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# switch computational backend for different model\nlinear_reg() |> \n  set_engine(\"lm\") |> \n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n```\n\n\n:::\n\n```{.r .cell-code}\n#  regularized regression is the glmnet model \nlinear_reg(penalty=1) |> \n  set_engine(\"glmnet\") |> \n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")\n```\n\n\n:::\n\n```{.r .cell-code}\n# To estimate with regularization, the second case, a Bayesian model can be fit using the rstanarm package:\nlinear_reg() |> \n  set_engine(\"stan\") |> \n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\nModel fit template:\nrstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n    weights = missing_arg(), family = stats::gaussian, refresh = 0)\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model = linear_reg() |> \n  set_engine(\"lm\") |> \n  translate()\n\nlm_model |> \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -313.623       -2.074        2.965  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_xy_fit <- \n  lm_model %>% \n  fit_xy(\n    x = ames_train %>% select(Longitude, Latitude),\n    y = ames_train %>% pull(Sale_Price)\n  )\n\nlm_xy_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -313.623       -2.074        2.965  \n```\n\n\n:::\n:::\n\n\n### Tree Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_forest(trees = 1000, min_n = 5) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\") %>% \n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n    verbose = FALSE, seed = sample.int(10^5, 1))\n```\n\n\n:::\n:::\n\n\n## Capture Model Results\n\n### Raw original way (useful to check og documentation)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_form_fit <- \n  lm_model %>% \n  # Recall that Sale_Price has been pre-logged\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_form_fit %>% extract_fit_engine() %>% vcov()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            (Intercept)    Longitude     Latitude\n(Intercept)  273.852441  2.052444651 -1.942540743\nLongitude      2.052445  0.021122353 -0.001771692\nLatitude      -1.942541 -0.001771692  0.042265807\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_res <- \n  lm_form_fit %>% \n  extract_fit_engine() %>% \n  summary()\n\n# The model coefficient table is accessible via the `coef` method.\nparam_est <- coef(model_res)\nclass(param_est)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"matrix\" \"array\" \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparam_est\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -313.622655 16.5484876 -18.95174 5.089063e-73\nLongitude     -2.073783  0.1453353 -14.26896 8.697331e-44\nLatitude       2.965370  0.2055865  14.42395 1.177304e-44\n```\n\n\n:::\n:::\n\n\n### The `Tidy` ecosystem for model result\n\nWhat's good about tidy is so you can reuse model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lm_form_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -314.      16.5       -19.0 5.09e-73\n2 Longitude      -2.07     0.145     -14.3 8.70e-44\n3 Latitude        2.97     0.206      14.4 1.18e-44\n```\n\n\n:::\n:::\n\n\n# Model Workflow\n\nChapter Link: [workflows](https://www.tmwr.org/workflows)\n\nSimilar in Python or Spark this is called \"pipeline\"\n\n1.  Initiate a workflow use `workflow()`\n2.  Add whatever model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ndata(ames)\names <- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\nlm_model <- linear_reg() %>% set_engine(\"lm\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## set up parsnip linear model\nlm_model <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n\n## add this model to workflow (pipline)\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model)\n\nlm_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: None\nModel: linear_reg()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\nR formula is now used as a \"pre-processor\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_wflow <- \n  lm_wflow %>% \n  add_formula(Sale_Price ~ Longitude + Latitude)\n\nlm_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n### Update Fomula\n\nIt is possible to update the formula to this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% update_formula(Sale_Price ~ Longitude)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_wflow <- \n  lm_wflow %>% \n  remove_formula() %>% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\nlm_wflow\n```\n:::\n\n\n### The Role of Formula:\n\n-   inline transformations (e.g., `log(x)`);\n-   creating dummy variable columns;\n-   creating interactions or other column expansions\n\n### Formula is Package Depend:\n\nYou have to go through each model one by one to see what type pre-processing are required for each different model.\n\n> -   Most packages for **tree-based models** use the formula interface but **do not encode the categorical predictors** as dummy variables.\n> -   Packages can use **special inline functions** that tell the model function how to treat the predictor in the analysis. *For example*, in survival analysis models, a formula term such as *`strata(site)`* would indicate that the column site is a stratification variable. This means it should not be treated as a regular predictor and does not have a corresponding location parameter estimate in the model.\n> -   A few R packages have extended the formula in ways that base R functions cannot parse or execute. In multilevel models (e.g., mixed models or hierarchical Bayesian models), a model term such as (week \\| subject) indicates that the column week is a random effect that has different slope parameter estimates for each value of the subject column.\n>\n> A workflow is a general purpose interface. When `add_formula()` is used, how should the workflow preprocess the data? Since the pre-processing is model dependent, **workflows** attempts to emulate what the underlying model would do whenever possible. If it is not possible, the formula processing should not do anything to the columns used in the formula. Let’s look at this in more detail.\n\n### Special Formula/In-line Function\n\nBecause standard R methods cannot properly process this formula this will result in error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nlibrary(nlme)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Orthodont\")\nlmer(distance ~ Sex + (age | Subject), data = Orthodont)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ Sex + (age | Subject)\n   Data: Orthodont\nREML criterion at convergence: 471.1635\nRandom effects:\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 7.3912        \n          age         0.6943   -0.97\n Residual             1.3100        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)    SexFemale  \n     24.517       -2.145  \n```\n\n\n:::\n\n```{.r .cell-code}\nmodel.matrix(distance ~ Sex + (age | Subject), data = Orthodont)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in Ops.ordered(age, Subject): '|' is not meaningful for ordered factors\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept) SexFemale age | SubjectTRUE\nattr(,\"assign\")\n[1] 0 1 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$Sex\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$`age | Subject`\n[1] \"contr.treatment\"\n```\n\n\n:::\n:::\n\n\nHowever, use `add_model` or `add_variables` solve this problem\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(multilevelmod)\n\nmultilevel_spec <- linear_reg() %>% set_engine(\"lmer\")\n\nmultilevel_workflow <- \n  workflow() %>% \n  # Pass the data along as-is: \n  add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %>% \n  add_model(multilevel_spec, \n            # This formula is given to the model\n            formula = distance ~ Sex + (age | Subject))\n\nmultilevel_fit <- fit(multilevel_workflow, data = Orthodont)\nmultilevel_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Variables\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nOutcomes: distance\nPredictors: c(Sex, age, Subject)\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ Sex + (age | Subject)\n   Data: data\nREML criterion at convergence: 471.1635\nRandom effects:\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 7.3912        \n          age         0.6943   -0.97\n Residual             1.3100        \nNumber of obs: 108, groups:  Subject, 27\nFixed Effects:\n(Intercept)    SexFemale  \n     24.517       -2.145  \n```\n\n\n:::\n:::\n\n\n### Use Multiple Model at Once\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlocation <- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood)\n  \n  \nlibrary(workflowsets)\n\nlocation_models <- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A workflow set/tibble: 4 × 4\n  wflow_id        info             option    result    \n  <chr>           <list>           <list>    <list>    \n1 longitude_lm    <tibble [1 × 4]> <opts[0]> <list [0]>\n2 latitude_lm     <tibble [1 × 4]> <opts[0]> <list [0]>\n3 coords_lm       <tibble [1 × 4]> <opts[0]> <list [0]>\n4 neighborhood_lm <tibble [1 × 4]> <opts[0]> <list [0]>\n```\n\n\n:::\n:::\n\n\nIf you ever want to fit these model you have to use \\`purrr::map\\` which is actually intuitive for R user.\n\nRight now these data.frames are all empty.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlocation_models <-\n   location_models %>%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\nlocation_models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A workflow set/tibble: 4 × 5\n  wflow_id        info             option    result     fit       \n  <chr>           <list>           <list>    <list>     <list>    \n1 longitude_lm    <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n2 latitude_lm     <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n3 coords_lm       <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n4 neighborhood_lm <tibble [1 × 4]> <opts[0]> <list [0]> <workflow>\n```\n\n\n:::\n:::\n\n\n## Evaluate Test Set use \\`last_fit()\\` method\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_lm_res <- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [2342/588]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n\n\n:::\n:::\n\n\n> ...the modeling process encompasses more than just estimating the parameters of an algorithm that connects predictors to an outcome. This process also includes pre-processing steps and operations taken after a model is fit. We introduced a concept called a model workflow that can capture the important components of the modeling process. Multiple workflows can also be created inside of a workflow set. The `last_fit()` function is convenient for fitting a final model to the training set and evaluating with the test set.\n>\n> For the Ames data, the related code that we’ll see used again is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ndata(ames)\n\n## normalise y\names <- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## split data\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\n## linear models\nlm_model <- linear_reg() %>% set_engine(\"lm\")\n\n## validating result\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model) %>% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\nlm_fit <- fit(lm_wflow, ames_train)\n```\n:::\n\n\n\n# Feature Engineering with Receipy\n\n\n::: {.callout-tip title=\"Syntax to use with `recipe`\"}\n\nUSAGE:\n\n- Start with `recipe()` function call \n- begin with a series of `step_*`\n\n```.r\n## create a receipy object\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_dummy(all_nominal_predictors())\n\n## add a receipy\nlm_wflow %>% \n  add_recipe(simple_ames)\n```\n\n::: \n\n\n### Compare Receipy with Standard Linear Model with formula\n\n> When this function is executed, the data are converted from a data frame to \n**a numeric design matrix** (also called a model matrix) and then the least squares\nmethod is used to estimate parameters. \n\nA Standard Linear Model:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames)\n```\n:::\n\n\nUse Receipy:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels) # Includes the recipes package\ntidymodels_prefer()\n\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_dummy(all_nominal_predictors())\nsimple_ames\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Log transformation on: Gr_Liv_Area\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal_predictors()\n```\n\n\n:::\n\n```{.r .cell-code}\n#> \n#> ── Recipe ───────────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:   1\n#> predictor: 4\n#> \n#> ── Operations\n#> • Log transformation on: Gr_Liv_Area\n#> • Dummy variables from: all_nominal_predictors()\n```\n:::\n\n\n\n**Receipy is more verbal but more flexible use of formula**:\n\nOkay why not use formula? \n\n> \n- These computations can be recycled across models since they are not tightly coupled to the modeling function.\n- A recipe enables a **broader set of data processing choices** than formulas can offer.\n- The syntax can be very compact. For example, `all_nominal_predictors()` can be used to capture many variables for specific types of processing while a formula would require each to be explicitly listed.\n- All data processing can be captured **in a single R object** instead of in scripts that are repeated, or even spread across different files.\n\n\n**Note on removing existing pre-processor before adding receipy**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_wflow %>% \n  add_recipe(simple_ames)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `add_recipe()`:\n! A recipe cannot be added when a formula already exists.\n```\n\n\n:::\n:::\n\n\nYou will have to remove existing preprocessor before adding recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_wflow <- \n  lm_wflow %>% \n  remove_variables() %>% \n  add_recipe(simple_ames)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The workflow has no variables preprocessor to remove.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `add_recipe()`:\n! A recipe cannot be added when a formula already exists.\n```\n\n\n:::\n\n```{.r .cell-code}\nlm_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSale_Price ~ Longitude + Latitude\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n### Typical Pre-Processing in Statiscs\n\n::: {.callout-note}\n\nThis section include two typical treatment\n\n- For nominal value, you may consider drop **low-frequency** terms `step_other`.\n- The second is for **interaction terms**. Combined effect is higher than addictive\nspecify by using `step_interact\n- spine function (non-linear relationship), typically used for coordinate `step_ns`\n  - I like to think of spine function as a stretching sheet.\n- PCA feature extraction technique (use `step_normalise`)\n\n:::\n\n#### Consider Encode Normial Values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = ames_train |> \n  count(Neighborhood) |> \n  mutate(freqency = n / sum(n))\n\n\nhighest_n_at_0.01 = d |> \n  filter(freqency <= 0.01) |> \n  filter(n == max(n)) |> \n  pull(n)\n  \nd |> \n  ggplot(aes(y=Neighborhood,x=n)) + \n  geom_col() +\n  gghighlight::gghighlight(n <= highest_n_at_0.01) + \n  ggtitle(\"These low frequency variables can be problematic\")\n```\n\n::: {.cell-output-display}\n![](01_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n- Norminal Values: Consider chunk low frequency category into others\nthis step you would use `step_other`;\n- `step_dummy(all_nominal_predictors)`;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\n#### Consider Interation Terms: Variable Can Interact with One and Other\n\n>  Interactions are defined in terms of their effect on the outcome and can be **combinations of different types of data** (e.g., numeric, categorical, etc). [Chapter 7 of M. Kuhn and Johnson (2020)](https://bookdown.org/max/FES/detecting-interaction-effects.html) discusses interactions and how to detect them in greater detail.\n\n>  ... two or more predictors are said to interact if their combined effect is different (less or greater) than what we would expect if we were to add the impact of each of their effects when considered alone.\n\n::: {.callout-tips}\n\nConsider Interaction as `group_by` recalculate regression in \n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) + \n  geom_point(alpha = .2) + \n  facet_wrap(~ Bldg_Type) + \n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"lightblue\") + \n  scale_x_log10() + \n  scale_y_log10() + \n  labs(x = \"Gross Living Area\", y = \"Sale Price (USD)\")\n```\n\n::: {.cell-output-display}\n![](01_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  # Gr_Liv_Area is on the log scale from a previous step\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") )\n```\n:::\n\n\n#### Spine Function (Non-Linear Relationship)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(splines)\n\nplot_smoother <- function(deg_free) {\n  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) + \n    geom_point(alpha = .2) + \n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(title = paste(deg_free, \"Spline Terms\"),\n         y = \"Sale Price (USD)\") +\n    theme_minimal()\n}\n\n# plot_smoother(2) \n( plot_smoother(2) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) )\n```\n\n::: {.cell-output-display}\n![](01_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\nThe example use case here is for coordinates, which is for \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, deg_free = 20)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Log transformation on: Gr_Liv_Area\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Collapsing factor levels for: Neighborhood\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Natural splines on: Latitude\n```\n\n\n:::\n:::\n\n\n\n#### Feature Extraction (Dimension Reduction Techniques)\n\nThe typical one you will see is PCA, But there exists more dimension reduction \ntechnique for example:\n\n- ICA Independent Component Analysis\n- NNMF Non-Negative Matrix Factorization\n- Multidimensional Scaling (MDS)\n- Uniform Manifold Approximation and Projection(UMAP)\n\n#### Row Sampling Steps\n\nThis is a technique said to improve distribution but not performance.\n\n#### Natual Language Sampling\n\n# Model Effectiveness Measurement\n\n::: {.callout-note title=\"Chapter Extract\"}\n \nThe typical statistical analysis workflow is analyzing different performance matrix \ngiven data.But in sum, you should think of measurement as these: \n\n- Accuracy Measurement (rmse)\n- Effectiveness Measurement (rsquare)\n- Implication Measuremnt (rsq)\n\nClassical Measure for Normalized value are these three: \n\n- `rmse`\n- `rsq`\n- `mae`\n\nFor **Binary Data** there are: \n\n- `conf_mat` confusion matrix\n- `accuracy`\n- `mcc` Matthews correlation coefficient\n- `f_meas` F1 metric\n\nWhen you use predicted probabilities as input rather than hard class predictor,\nthe one matrix is `ROC` (ding ding ding!)\n\n- `roc_curve`\n\nThere is a use-full function `autoplot` let you do this.\n\n:::\n\nUltimately this gives you the power to compare different performance matrix easily\n\n## Yardstick Basic Usage\n\nYardstick is tool that produce performance matrix with consistent interface.\n\n```.r\n## Create a prediction frame\names_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))\names_test_res\n#> # A tibble: 588 × 1\n#>   .pred\n#>   <dbl>\n#> 1  5.07\n#> 2  5.31\n\n\n## Bind prediction with Actual value\names_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))\names_test_res\n#> # A tibble: 588 × 2\n#>   .pred Sale_Price\n#>   <dbl>      <dbl>\n#> 1  5.07       5.02\n#> 2  5.31       5.39\n\n## YARDSTICK!! given a dataframe just do these two\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n\n## compare multiple matrix\names_metrics <- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n```\n\n\n",
    "supporting": [
      "01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}